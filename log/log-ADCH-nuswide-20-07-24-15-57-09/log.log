root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000, pEpsilon=0.5, pGamma=2, pLambda=200)
root: INFO: 12
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 257.9432]
root: INFO: [Iteration:   1/ 50][Train Loss: 216.2846]
root: INFO: [Iteration:   2/ 50][Train Loss: 188.9885]
root: INFO: [Iteration:   3/ 50][Train Loss: 193.3647]
root: INFO: [Iteration:   4/ 50][Train Loss: 182.6604]
root: INFO: [Iteration:   5/ 50][Train Loss: 162.2156]
root: INFO: [Iteration:   6/ 50][Train Loss: 178.0424]
root: INFO: [Iteration:   7/ 50][Train Loss: 168.4274]
root: INFO: [Iteration:   8/ 50][Train Loss: 170.8663]
root: INFO: [Iteration:   9/ 50][Train Loss: 165.5528]
root: INFO: [Iteration:  10/ 50][Train Loss: 170.0350]
root: INFO: [Iteration:  11/ 50][Train Loss: 179.5571]
root: INFO: [Iteration:  12/ 50][Train Loss: 172.3075]
root: INFO: [Iteration:  13/ 50][Train Loss: 175.0260]
root: INFO: [Iteration:  14/ 50][Train Loss: 177.5393]
root: INFO: [Iteration:  15/ 50][Train Loss: 176.6836]
root: INFO: [Iteration:  16/ 50][Train Loss: 176.3935]
root: INFO: [Iteration:  17/ 50][Train Loss: 176.0936]
root: INFO: [Iteration:  18/ 50][Train Loss: 176.7293]
root: INFO: [Iteration:  19/ 50][Train Loss: 176.0253]
root: INFO: [Iteration:  20/ 50][Train Loss: 176.5413]
root: INFO: [Iteration:  21/ 50][Train Loss: 183.4111]
root: INFO: [Iteration:  22/ 50][Train Loss: 179.3773]
root: INFO: [Iteration:  23/ 50][Train Loss: 187.9184]
root: INFO: [Iteration:  24/ 50][Train Loss: 181.8513]
root: INFO: [Iteration:  25/ 50][Train Loss: 178.2166]
root: INFO: [Iteration:  26/ 50][Train Loss: 182.5218]
root: INFO: [Iteration:  27/ 50][Train Loss: 175.0048]
root: INFO: [Iteration:  28/ 50][Train Loss: 189.6717]
root: INFO: [Iteration:  29/ 50][Train Loss: 178.5956]
root: INFO: [Iteration:  30/ 50][Train Loss: 182.4265]
root: INFO: [Iteration:  31/ 50][Train Loss: 191.1389]
root: INFO: [Iteration:  32/ 50][Train Loss: 184.4650]
root: INFO: [Iteration:  33/ 50][Train Loss: 181.7641]
root: INFO: [Iteration:  34/ 50][Train Loss: 181.4907]
root: INFO: [Iteration:  35/ 50][Train Loss: 185.0126]
root: INFO: [Iteration:  36/ 50][Train Loss: 188.5563]
root: INFO: [Iteration:  37/ 50][Train Loss: 189.7735]
root: INFO: [Iteration:  38/ 50][Train Loss: 191.4285]
root: INFO: [Iteration:  39/ 50][Train Loss: 176.9065]
root: INFO: [Iteration:  40/ 50][Train Loss: 187.0915]
root: INFO: [Iteration:  41/ 50][Train Loss: 188.2498]
root: INFO: [Iteration:  42/ 50][Train Loss: 180.0827]
root: INFO: [Iteration:  43/ 50][Train Loss: 187.6106]
root: INFO: [Iteration:  44/ 50][Train Loss: 187.1020]
root: INFO: [Iteration:  45/ 50][Train Loss: 179.3211]
root: INFO: [Iteration:  46/ 50][Train Loss: 185.1311]
root: INFO: [Iteration:  47/ 50][Train Loss: 181.9680]
root: INFO: [Iteration:  48/ 50][Train Loss: 185.8398]
root: INFO: [Iteration:  49/ 50][Train Loss: 187.7874]
root: INFO: [Evaluation: mAP: 0.7690, top-5000 mAP: 0.8452]
root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000, pEpsilon=0.5, pGamma=2, pLambda=200)
root: INFO: 24
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 1345.7568]
root: INFO: [Iteration:   1/ 50][Train Loss: 1083.7900]
root: INFO: [Iteration:   2/ 50][Train Loss: 1015.6946]
root: INFO: [Iteration:   3/ 50][Train Loss: 1016.7504]
root: INFO: [Iteration:   4/ 50][Train Loss: 1025.0507]
root: INFO: [Iteration:   5/ 50][Train Loss: 936.3264]
root: INFO: [Iteration:   6/ 50][Train Loss: 945.3483]
root: INFO: [Iteration:   7/ 50][Train Loss: 927.0067]
root: INFO: [Iteration:   8/ 50][Train Loss: 958.2285]
root: INFO: [Iteration:   9/ 50][Train Loss: 927.1865]
root: INFO: [Iteration:  10/ 50][Train Loss: 929.5901]
root: INFO: [Iteration:  11/ 50][Train Loss: 925.6665]
root: INFO: [Iteration:  12/ 50][Train Loss: 932.3722]
root: INFO: [Iteration:  13/ 50][Train Loss: 937.9230]
root: INFO: [Iteration:  14/ 50][Train Loss: 935.8685]
root: INFO: [Iteration:  15/ 50][Train Loss: 918.6259]
root: INFO: [Iteration:  16/ 50][Train Loss: 919.1719]
root: INFO: [Iteration:  17/ 50][Train Loss: 909.7407]
root: INFO: [Iteration:  18/ 50][Train Loss: 925.9664]
root: INFO: [Iteration:  19/ 50][Train Loss: 934.4959]
root: INFO: [Iteration:  20/ 50][Train Loss: 899.4865]
root: INFO: [Iteration:  21/ 50][Train Loss: 920.4739]
root: INFO: [Iteration:  22/ 50][Train Loss: 946.9912]
root: INFO: [Iteration:  23/ 50][Train Loss: 959.9312]
root: INFO: [Iteration:  24/ 50][Train Loss: 921.3613]
root: INFO: [Iteration:  25/ 50][Train Loss: 935.9625]
root: INFO: [Iteration:  26/ 50][Train Loss: 928.7835]
root: INFO: [Iteration:  27/ 50][Train Loss: 935.8790]
root: INFO: [Iteration:  28/ 50][Train Loss: 945.8218]
root: INFO: [Iteration:  29/ 50][Train Loss: 959.3311]
root: INFO: [Iteration:  30/ 50][Train Loss: 924.5594]
root: INFO: [Iteration:  31/ 50][Train Loss: 977.0323]
root: INFO: [Iteration:  32/ 50][Train Loss: 924.3154]
root: INFO: [Iteration:  33/ 50][Train Loss: 915.3217]
root: INFO: [Iteration:  34/ 50][Train Loss: 956.5789]
root: INFO: [Iteration:  35/ 50][Train Loss: 952.1259]
root: INFO: [Iteration:  36/ 50][Train Loss: 928.3244]
root: INFO: [Iteration:  37/ 50][Train Loss: 929.8930]
root: INFO: [Iteration:  38/ 50][Train Loss: 958.3071]
root: INFO: [Iteration:  39/ 50][Train Loss: 980.7290]
root: INFO: [Iteration:  40/ 50][Train Loss: 954.7958]
root: INFO: [Iteration:  41/ 50][Train Loss: 949.2440]
root: INFO: [Iteration:  42/ 50][Train Loss: 946.1348]
root: INFO: [Iteration:  43/ 50][Train Loss: 942.7789]
root: INFO: [Iteration:  44/ 50][Train Loss: 946.8328]
root: INFO: [Iteration:  45/ 50][Train Loss: 962.7578]
root: INFO: [Iteration:  46/ 50][Train Loss: 954.4544]
root: INFO: [Iteration:  47/ 50][Train Loss: 947.0183]
root: INFO: [Iteration:  48/ 50][Train Loss: 910.6458]
root: INFO: [Iteration:  49/ 50][Train Loss: 945.8415]
root: INFO: [Evaluation: mAP: 0.7736, top-5000 mAP: 0.8504]
root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000, pEpsilon=0.5, pGamma=2, pLambda=200)
root: INFO: 32
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 2280.4175]
root: INFO: [Iteration:   1/ 50][Train Loss: 2173.6826]
root: INFO: [Iteration:   2/ 50][Train Loss: 2192.7171]
root: INFO: [Iteration:   3/ 50][Train Loss: 2073.4671]
root: INFO: [Iteration:   4/ 50][Train Loss: 1965.9910]
root: INFO: [Iteration:   5/ 50][Train Loss: 2048.8911]
root: INFO: [Iteration:   6/ 50][Train Loss: 2031.4610]
root: INFO: [Iteration:   7/ 50][Train Loss: 2062.5662]
root: INFO: [Iteration:   8/ 50][Train Loss: 1929.8149]
root: INFO: [Iteration:   9/ 50][Train Loss: 1979.3727]
root: INFO: [Iteration:  10/ 50][Train Loss: 1993.3081]
root: INFO: [Iteration:  11/ 50][Train Loss: 1917.9343]
root: INFO: [Iteration:  12/ 50][Train Loss: 1905.7424]
root: INFO: [Iteration:  13/ 50][Train Loss: 1903.6214]
root: INFO: [Iteration:  14/ 50][Train Loss: 1900.1811]
root: INFO: [Iteration:  15/ 50][Train Loss: 1865.3812]
root: INFO: [Iteration:  16/ 50][Train Loss: 1904.4079]
root: INFO: [Iteration:  17/ 50][Train Loss: 1843.9041]
root: INFO: [Iteration:  18/ 50][Train Loss: 1835.6763]
root: INFO: [Iteration:  19/ 50][Train Loss: 1789.1101]
root: INFO: [Iteration:  20/ 50][Train Loss: 1786.7179]
root: INFO: [Iteration:  21/ 50][Train Loss: 1806.9400]
root: INFO: [Iteration:  22/ 50][Train Loss: 1813.7812]
root: INFO: [Iteration:  23/ 50][Train Loss: 1827.2643]
root: INFO: [Iteration:  24/ 50][Train Loss: 1910.8128]
root: INFO: [Iteration:  25/ 50][Train Loss: 1884.6951]
root: INFO: [Iteration:  26/ 50][Train Loss: 1847.3038]
root: INFO: [Iteration:  27/ 50][Train Loss: 1907.0154]
root: INFO: [Iteration:  28/ 50][Train Loss: 1896.3753]
root: INFO: [Iteration:  29/ 50][Train Loss: 1860.2827]
root: INFO: [Iteration:  30/ 50][Train Loss: 1850.2074]
root: INFO: [Iteration:  31/ 50][Train Loss: 1897.4137]
root: INFO: [Iteration:  32/ 50][Train Loss: 1895.6026]
root: INFO: [Iteration:  33/ 50][Train Loss: 1835.6649]
root: INFO: [Iteration:  34/ 50][Train Loss: 1916.5528]
root: INFO: [Iteration:  35/ 50][Train Loss: 1878.6706]
root: INFO: [Iteration:  36/ 50][Train Loss: 1909.1797]
root: INFO: [Iteration:  37/ 50][Train Loss: 1912.9700]
root: INFO: [Iteration:  38/ 50][Train Loss: 1870.9413]
root: INFO: [Iteration:  39/ 50][Train Loss: 1935.3384]
root: INFO: [Iteration:  40/ 50][Train Loss: 1941.8656]
root: INFO: [Iteration:  41/ 50][Train Loss: 1898.1488]
root: INFO: [Iteration:  42/ 50][Train Loss: 1922.7448]
root: INFO: [Iteration:  43/ 50][Train Loss: 1880.5125]
root: INFO: [Iteration:  44/ 50][Train Loss: 1891.6122]
root: INFO: [Iteration:  45/ 50][Train Loss: 1841.1046]
root: INFO: [Iteration:  46/ 50][Train Loss: 1919.3441]
root: INFO: [Iteration:  47/ 50][Train Loss: 1930.8436]
root: INFO: [Iteration:  48/ 50][Train Loss: 1890.5676]
root: INFO: [Iteration:  49/ 50][Train Loss: 1882.8551]
root: INFO: [Evaluation: mAP: 0.7656, top-5000 mAP: 0.8365]
root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000, pEpsilon=0.5, pGamma=2, pLambda=200)
root: INFO: 48
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 5927.2160]
root: INFO: [Iteration:   1/ 50][Train Loss: 5748.1646]
root: INFO: [Iteration:   2/ 50][Train Loss: 5709.8989]
root: INFO: [Iteration:   3/ 50][Train Loss: 5699.9867]
root: INFO: [Iteration:   4/ 50][Train Loss: 5844.7681]
root: INFO: [Iteration:   5/ 50][Train Loss: 5700.2649]
root: INFO: [Iteration:   6/ 50][Train Loss: 5348.6949]
root: INFO: [Iteration:   7/ 50][Train Loss: 5468.5254]
root: INFO: [Iteration:   8/ 50][Train Loss: 5475.3930]
root: INFO: [Iteration:   9/ 50][Train Loss: 5357.6207]
root: INFO: [Iteration:  10/ 50][Train Loss: 5323.5815]
root: INFO: [Iteration:  11/ 50][Train Loss: 5423.0300]
root: INFO: [Iteration:  12/ 50][Train Loss: 5391.0127]
root: INFO: [Iteration:  13/ 50][Train Loss: 5219.5761]
root: INFO: [Iteration:  14/ 50][Train Loss: 5325.3433]
root: INFO: [Iteration:  15/ 50][Train Loss: 5288.7924]
root: INFO: [Iteration:  16/ 50][Train Loss: 5306.2818]
root: INFO: [Iteration:  17/ 50][Train Loss: 5294.4418]
root: INFO: [Iteration:  18/ 50][Train Loss: 5243.5116]
root: INFO: [Iteration:  19/ 50][Train Loss: 5078.9641]
root: INFO: [Iteration:  20/ 50][Train Loss: 5093.4396]
root: INFO: [Iteration:  21/ 50][Train Loss: 5288.1919]
root: INFO: [Iteration:  22/ 50][Train Loss: 5307.4150]
root: INFO: [Iteration:  23/ 50][Train Loss: 5172.3403]
root: INFO: [Iteration:  24/ 50][Train Loss: 5331.0196]
root: INFO: [Iteration:  25/ 50][Train Loss: 5359.9521]
root: INFO: [Iteration:  26/ 50][Train Loss: 5264.7042]
root: INFO: [Iteration:  27/ 50][Train Loss: 5426.4084]
root: INFO: [Iteration:  28/ 50][Train Loss: 5348.3743]
root: INFO: [Iteration:  29/ 50][Train Loss: 5360.2708]
root: INFO: [Iteration:  30/ 50][Train Loss: 5272.9538]
root: INFO: [Iteration:  31/ 50][Train Loss: 5263.7320]
root: INFO: [Iteration:  32/ 50][Train Loss: 5156.5972]
root: INFO: [Iteration:  33/ 50][Train Loss: 5329.7623]
root: INFO: [Iteration:  34/ 50][Train Loss: 5308.2502]
root: INFO: [Iteration:  35/ 50][Train Loss: 5342.5780]
root: INFO: [Iteration:  36/ 50][Train Loss: 5358.6943]
root: INFO: [Iteration:  37/ 50][Train Loss: 5280.0023]
root: INFO: [Iteration:  38/ 50][Train Loss: 5174.8062]
root: INFO: [Iteration:  39/ 50][Train Loss: 5398.3936]
root: INFO: [Iteration:  40/ 50][Train Loss: 5431.1654]
root: INFO: [Iteration:  41/ 50][Train Loss: 5234.6170]
root: INFO: [Iteration:  42/ 50][Train Loss: 5375.5178]
root: INFO: [Iteration:  43/ 50][Train Loss: 5341.8550]
root: INFO: [Iteration:  44/ 50][Train Loss: 5216.0179]
root: INFO: [Iteration:  45/ 50][Train Loss: 5427.4138]
root: INFO: [Iteration:  46/ 50][Train Loss: 5343.5055]
root: INFO: [Iteration:  47/ 50][Train Loss: 5388.7535]
root: INFO: [Iteration:  48/ 50][Train Loss: 5243.8139]
root: INFO: [Iteration:  49/ 50][Train Loss: 5225.0277]
root: INFO: [Evaluation: mAP: 0.6960, top-5000 mAP: 0.7450]
