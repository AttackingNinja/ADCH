root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000, pEpsilon=0.5, pGamma=2, pLambda=200)
root: INFO: 12
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 135.5255]
root: INFO: [Iteration:   1/ 50][Train Loss: 135.5298]
root: INFO: [Iteration:   2/ 50][Train Loss: 134.3275]
root: INFO: [Iteration:   3/ 50][Train Loss: 135.4443]
root: INFO: [Iteration:   4/ 50][Train Loss: 135.4090]
root: INFO: [Iteration:   5/ 50][Train Loss: 129.6883]
root: INFO: [Iteration:   6/ 50][Train Loss: 128.2414]
root: INFO: [Iteration:   7/ 50][Train Loss: 124.3310]
root: INFO: [Iteration:   8/ 50][Train Loss: 108.3007]
root: INFO: [Iteration:   9/ 50][Train Loss: 98.3542]
root: INFO: [Iteration:  10/ 50][Train Loss: 88.4682]
root: INFO: [Iteration:  11/ 50][Train Loss: 89.0099]
root: INFO: [Iteration:  12/ 50][Train Loss: 89.4508]
root: INFO: [Iteration:  13/ 50][Train Loss: 89.8499]
root: INFO: [Iteration:  14/ 50][Train Loss: 89.0798]
root: INFO: [Iteration:  15/ 50][Train Loss: 88.7273]
root: INFO: [Iteration:  16/ 50][Train Loss: 89.0884]
root: INFO: [Iteration:  17/ 50][Train Loss: 89.8728]
root: INFO: [Iteration:  18/ 50][Train Loss: 89.6221]
root: INFO: [Iteration:  19/ 50][Train Loss: 87.7840]
root: INFO: [Iteration:  20/ 50][Train Loss: 88.1089]
root: INFO: [Iteration:  21/ 50][Train Loss: 88.0648]
root: INFO: [Iteration:  22/ 50][Train Loss: 89.4519]
root: INFO: [Iteration:  23/ 50][Train Loss: 88.9742]
root: INFO: [Iteration:  24/ 50][Train Loss: 88.8595]
root: INFO: [Iteration:  25/ 50][Train Loss: 88.9026]
root: INFO: [Iteration:  26/ 50][Train Loss: 90.1293]
root: INFO: [Iteration:  27/ 50][Train Loss: 90.6410]
root: INFO: [Iteration:  28/ 50][Train Loss: 87.7786]
root: INFO: [Iteration:  29/ 50][Train Loss: 90.0978]
root: INFO: [Iteration:  30/ 50][Train Loss: 88.8397]
root: INFO: [Iteration:  31/ 50][Train Loss: 88.6715]
root: INFO: [Iteration:  32/ 50][Train Loss: 89.1110]
root: INFO: [Iteration:  33/ 50][Train Loss: 89.2767]
root: INFO: [Iteration:  34/ 50][Train Loss: 90.6126]
root: INFO: [Iteration:  35/ 50][Train Loss: 89.1871]
root: INFO: [Iteration:  36/ 50][Train Loss: 89.9650]
root: INFO: [Iteration:  37/ 50][Train Loss: 88.0156]
root: INFO: [Iteration:  38/ 50][Train Loss: 89.0185]
root: INFO: [Iteration:  39/ 50][Train Loss: 88.6187]
root: INFO: [Iteration:  40/ 50][Train Loss: 90.3262]
root: INFO: [Iteration:  41/ 50][Train Loss: 89.5254]
root: INFO: [Iteration:  42/ 50][Train Loss: 88.7078]
root: INFO: [Iteration:  43/ 50][Train Loss: 89.0536]
root: INFO: [Iteration:  44/ 50][Train Loss: 90.0791]
root: INFO: [Iteration:  45/ 50][Train Loss: 90.3413]
root: INFO: [Iteration:  46/ 50][Train Loss: 88.1322]
root: INFO: [Iteration:  47/ 50][Train Loss: 90.0100]
root: INFO: [Iteration:  48/ 50][Train Loss: 89.1875]
root: INFO: [Iteration:  49/ 50][Train Loss: 90.2239]
root: INFO: [Evaluation: mAP: 0.7362, top-5000 mAP: 0.7581]
root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000, pEpsilon=0.5, pGamma=2, pLambda=200)
root: INFO: 24
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 673.9447]
root: INFO: [Iteration:   1/ 50][Train Loss: 602.6449]
root: INFO: [Iteration:   2/ 50][Train Loss: 540.9055]
root: INFO: [Iteration:   3/ 50][Train Loss: 506.3259]
root: INFO: [Iteration:   4/ 50][Train Loss: 488.1765]
root: INFO: [Iteration:   5/ 50][Train Loss: 491.7618]
root: INFO: [Iteration:   6/ 50][Train Loss: 492.4592]
root: INFO: [Iteration:   7/ 50][Train Loss: 491.0623]
root: INFO: [Iteration:   8/ 50][Train Loss: 496.4831]
root: INFO: [Iteration:   9/ 50][Train Loss: 484.8561]
root: INFO: [Iteration:  10/ 50][Train Loss: 479.4752]
root: INFO: [Iteration:  11/ 50][Train Loss: 486.4364]
root: INFO: [Iteration:  12/ 50][Train Loss: 475.3111]
root: INFO: [Iteration:  13/ 50][Train Loss: 474.6778]
root: INFO: [Iteration:  14/ 50][Train Loss: 475.5099]
root: INFO: [Iteration:  15/ 50][Train Loss: 476.3598]
root: INFO: [Iteration:  16/ 50][Train Loss: 473.5899]
root: INFO: [Iteration:  17/ 50][Train Loss: 467.3952]
root: INFO: [Iteration:  18/ 50][Train Loss: 473.2099]
root: INFO: [Iteration:  19/ 50][Train Loss: 472.9974]
root: INFO: [Iteration:  20/ 50][Train Loss: 473.7600]
root: INFO: [Iteration:  21/ 50][Train Loss: 480.3343]
root: INFO: [Iteration:  22/ 50][Train Loss: 471.9945]
root: INFO: [Iteration:  23/ 50][Train Loss: 468.5440]
root: INFO: [Iteration:  24/ 50][Train Loss: 464.8468]
root: INFO: [Iteration:  25/ 50][Train Loss: 465.4400]
root: INFO: [Iteration:  26/ 50][Train Loss: 459.4986]
root: INFO: [Iteration:  27/ 50][Train Loss: 448.4284]
root: INFO: [Iteration:  28/ 50][Train Loss: 453.1076]
root: INFO: [Iteration:  29/ 50][Train Loss: 447.8418]
root: INFO: [Iteration:  30/ 50][Train Loss: 451.3211]
root: INFO: [Iteration:  31/ 50][Train Loss: 447.3681]
root: INFO: [Iteration:  32/ 50][Train Loss: 455.7590]
root: INFO: [Iteration:  33/ 50][Train Loss: 448.4031]
root: INFO: [Iteration:  34/ 50][Train Loss: 449.9587]
root: INFO: [Iteration:  35/ 50][Train Loss: 448.9393]
root: INFO: [Iteration:  36/ 50][Train Loss: 448.0195]
root: INFO: [Iteration:  37/ 50][Train Loss: 448.5387]
root: INFO: [Iteration:  38/ 50][Train Loss: 448.6786]
root: INFO: [Iteration:  39/ 50][Train Loss: 449.1557]
root: INFO: [Iteration:  40/ 50][Train Loss: 445.4351]
root: INFO: [Iteration:  41/ 50][Train Loss: 448.9702]
root: INFO: [Iteration:  42/ 50][Train Loss: 450.6703]
root: INFO: [Iteration:  43/ 50][Train Loss: 448.8951]
root: INFO: [Iteration:  44/ 50][Train Loss: 454.8866]
root: INFO: [Iteration:  45/ 50][Train Loss: 448.3041]
root: INFO: [Iteration:  46/ 50][Train Loss: 445.6636]
root: INFO: [Iteration:  47/ 50][Train Loss: 448.0376]
root: INFO: [Iteration:  48/ 50][Train Loss: 441.7541]
root: INFO: [Iteration:  49/ 50][Train Loss: 449.2227]
root: INFO: [Evaluation: mAP: 0.7595, top-5000 mAP: 0.8117]
root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000, pEpsilon=0.5, pGamma=2, pLambda=200)
root: INFO: 32
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 1291.3211]
root: INFO: [Iteration:   1/ 50][Train Loss: 1153.8811]
root: INFO: [Iteration:   2/ 50][Train Loss: 1009.3651]
root: INFO: [Iteration:   3/ 50][Train Loss: 993.1969]
root: INFO: [Iteration:   4/ 50][Train Loss: 902.9713]
root: INFO: [Iteration:   5/ 50][Train Loss: 840.7431]
root: INFO: [Iteration:   6/ 50][Train Loss: 832.8897]
root: INFO: [Iteration:   7/ 50][Train Loss: 795.3832]
root: INFO: [Iteration:   8/ 50][Train Loss: 778.5085]
root: INFO: [Iteration:   9/ 50][Train Loss: 779.2434]
root: INFO: [Iteration:  10/ 50][Train Loss: 760.6756]
root: INFO: [Iteration:  11/ 50][Train Loss: 746.2161]
root: INFO: [Iteration:  12/ 50][Train Loss: 730.4521]
root: INFO: [Iteration:  13/ 50][Train Loss: 705.5900]
root: INFO: [Iteration:  14/ 50][Train Loss: 692.5512]
root: INFO: [Iteration:  15/ 50][Train Loss: 696.1053]
root: INFO: [Iteration:  16/ 50][Train Loss: 681.1473]
root: INFO: [Iteration:  17/ 50][Train Loss: 683.9501]
root: INFO: [Iteration:  18/ 50][Train Loss: 692.6340]
root: INFO: [Iteration:  19/ 50][Train Loss: 683.6975]
root: INFO: [Iteration:  20/ 50][Train Loss: 692.4680]
root: INFO: [Iteration:  21/ 50][Train Loss: 710.4855]
root: INFO: [Iteration:  22/ 50][Train Loss: 708.4289]
root: INFO: [Iteration:  23/ 50][Train Loss: 698.2555]
root: INFO: [Iteration:  24/ 50][Train Loss: 694.1581]
root: INFO: [Iteration:  25/ 50][Train Loss: 696.2647]
root: INFO: [Iteration:  26/ 50][Train Loss: 690.9501]
root: INFO: [Iteration:  27/ 50][Train Loss: 696.2592]
root: INFO: [Iteration:  28/ 50][Train Loss: 685.5934]
root: INFO: [Iteration:  29/ 50][Train Loss: 677.5035]
root: INFO: [Iteration:  30/ 50][Train Loss: 690.0396]
root: INFO: [Iteration:  31/ 50][Train Loss: 690.7904]
root: INFO: [Iteration:  32/ 50][Train Loss: 687.8460]
root: INFO: [Iteration:  33/ 50][Train Loss: 691.8122]
root: INFO: [Iteration:  34/ 50][Train Loss: 700.5410]
root: INFO: [Iteration:  35/ 50][Train Loss: 689.8102]
root: INFO: [Iteration:  36/ 50][Train Loss: 704.0788]
root: INFO: [Iteration:  37/ 50][Train Loss: 690.5082]
root: INFO: [Iteration:  38/ 50][Train Loss: 697.4868]
root: INFO: [Iteration:  39/ 50][Train Loss: 702.1066]
root: INFO: [Iteration:  40/ 50][Train Loss: 693.5644]
root: INFO: [Iteration:  41/ 50][Train Loss: 692.0178]
root: INFO: [Iteration:  42/ 50][Train Loss: 694.4936]
root: INFO: [Iteration:  43/ 50][Train Loss: 693.5750]
root: INFO: [Iteration:  44/ 50][Train Loss: 703.7903]
root: INFO: [Iteration:  45/ 50][Train Loss: 694.1477]
root: INFO: [Iteration:  46/ 50][Train Loss: 694.9944]
root: INFO: [Iteration:  47/ 50][Train Loss: 696.3251]
root: INFO: [Iteration:  48/ 50][Train Loss: 701.2867]
root: INFO: [Iteration:  49/ 50][Train Loss: 696.6277]
root: INFO: [Evaluation: mAP: 0.7343, top-5000 mAP: 0.8154]
root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000, pEpsilon=0.5, pGamma=2, pLambda=200)
root: INFO: 48
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 3413.5576]
root: INFO: [Iteration:   1/ 50][Train Loss: 3175.1168]
root: INFO: [Iteration:   2/ 50][Train Loss: 2829.4824]
root: INFO: [Iteration:   3/ 50][Train Loss: 2525.7763]
root: INFO: [Iteration:   4/ 50][Train Loss: 2185.9683]
root: INFO: [Iteration:   5/ 50][Train Loss: 1866.5912]
root: INFO: [Iteration:   6/ 50][Train Loss: 1811.3084]
root: INFO: [Iteration:   7/ 50][Train Loss: 1729.0575]
root: INFO: [Iteration:   8/ 50][Train Loss: 1619.2994]
root: INFO: [Iteration:   9/ 50][Train Loss: 1571.2120]
root: INFO: [Iteration:  10/ 50][Train Loss: 1513.6972]
root: INFO: [Iteration:  11/ 50][Train Loss: 1390.9562]
root: INFO: [Iteration:  12/ 50][Train Loss: 1397.6953]
root: INFO: [Iteration:  13/ 50][Train Loss: 1338.5526]
root: INFO: [Iteration:  14/ 50][Train Loss: 1266.1246]
root: INFO: [Iteration:  15/ 50][Train Loss: 1186.7000]
root: INFO: [Iteration:  16/ 50][Train Loss: 1202.4746]
root: INFO: [Iteration:  17/ 50][Train Loss: 1185.0542]
root: INFO: [Iteration:  18/ 50][Train Loss: 1183.4114]
root: INFO: [Iteration:  19/ 50][Train Loss: 1162.9594]
root: INFO: [Iteration:  20/ 50][Train Loss: 1144.7024]
root: INFO: [Iteration:  21/ 50][Train Loss: 1255.8757]
root: INFO: [Iteration:  22/ 50][Train Loss: 1227.0592]
root: INFO: [Iteration:  23/ 50][Train Loss: 1215.2125]
root: INFO: [Iteration:  24/ 50][Train Loss: 1239.9977]
root: INFO: [Iteration:  25/ 50][Train Loss: 1200.7433]
root: INFO: [Iteration:  26/ 50][Train Loss: 1150.8441]
root: INFO: [Iteration:  27/ 50][Train Loss: 1224.0598]
root: INFO: [Iteration:  28/ 50][Train Loss: 1221.8528]
root: INFO: [Iteration:  29/ 50][Train Loss: 1193.6930]
root: INFO: [Iteration:  30/ 50][Train Loss: 1181.8941]
root: INFO: [Iteration:  31/ 50][Train Loss: 1190.7485]
root: INFO: [Iteration:  32/ 50][Train Loss: 1185.5222]
root: INFO: [Iteration:  33/ 50][Train Loss: 1200.6902]
root: INFO: [Iteration:  34/ 50][Train Loss: 1232.9846]
root: INFO: [Iteration:  35/ 50][Train Loss: 1209.1469]
root: INFO: [Iteration:  36/ 50][Train Loss: 1210.3670]
root: INFO: [Iteration:  37/ 50][Train Loss: 1224.8424]
root: INFO: [Iteration:  38/ 50][Train Loss: 1216.8414]
root: INFO: [Iteration:  39/ 50][Train Loss: 1206.4692]
root: INFO: [Iteration:  40/ 50][Train Loss: 1231.1140]
root: INFO: [Iteration:  41/ 50][Train Loss: 1210.3427]
root: INFO: [Iteration:  42/ 50][Train Loss: 1199.0583]
root: INFO: [Iteration:  43/ 50][Train Loss: 1201.2333]
root: INFO: [Iteration:  44/ 50][Train Loss: 1203.6974]
root: INFO: [Iteration:  45/ 50][Train Loss: 1220.3259]
root: INFO: [Iteration:  46/ 50][Train Loss: 1218.7182]
root: INFO: [Iteration:  47/ 50][Train Loss: 1157.1879]
root: INFO: [Iteration:  48/ 50][Train Loss: 1207.9199]
root: INFO: [Iteration:  49/ 50][Train Loss: 1215.9608]
root: INFO: [Evaluation: mAP: 0.8287, top-5000 mAP: 0.7735]
